# NeuralNetwork-from-Scratch
# ğŸ§  Neural Network from Scratch â€” With Full Math Intuition

This repository contains a **from-scratch implementation of a fully connected neural network**, built using only **Python and NumPy** â€” no PyTorch, TensorFlow, or external ML libraries. Every line of code is backed by **hand-derived math**, making this a great educational project for anyone diving into deep learning.

---

## ğŸš§ Why I Built This

Before diving into libraries like PyTorch or TensorFlow, I wanted to understand whatâ€™s **under the hood** â€” how neural networks actually work, how gradients flow, and how backpropagation updates happen step by step.

This repo represents my journey from **"just using libraries"** to **"building the machinery myself"**.

---

## ğŸ” Features

- âœ… Manual forward propagation  
- âœ… Hand-coded backpropagation (no autograd!)  
- âœ… Fully vectorized implementation using NumPy  
- âœ… Support for any number of layers  
- âœ… Binary classification demo task  
- âœ… Full mathematical derivation of:
  - Linear transformations (`Z = WÂ·X + b`)
  - Activation functions (Sigmoid, ReLU)
  - Loss functions (MSE, Binary Cross-Entropy)
  - Gradient Descent updates
- âœ… Jupyter Notebook with visual + math walkthrough

---

## ğŸ§® Math Included

This isn't just code â€” it includes **step-by-step derivations** of:

- ğŸŸ¡ Chain rule for backpropagation  
- ğŸŸ¢ Partial derivatives w.r.t. weights and biases  
- ğŸ”µ Activation gradients  
- ğŸŸ  Loss gradients  
- ğŸŸ£ Update rules: `Î¸ = Î¸ - Î± âˆ‡L(Î¸)`

These are explained clearly in the [Jupyter Notebook](./notebook.ipynb) and markdown cells.

---
